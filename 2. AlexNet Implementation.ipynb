{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. calling module\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf 2.0 부터는 session방식보다 eager 방식을 쓰는 것을 권장한다고 한다. 그래서 tf.enable_eager_execution()을 써 주어야 한다고 한다. -> 공부가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. parameter setting\n",
    "learning_rate = 0.001\n",
    "training_epochs = 90\n",
    "batch_size = 64\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. load_data\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    #cifar학습 시켰던 실습 참고해서\n",
    "    \n",
    "    #이미지 타입 변환\n",
    "    \n",
    "    #이미지 차원 변환\n",
    "    \n",
    "    #이미지의 라벨 추출\n",
    "    \n",
    "    #이미지 data Augmentation\n",
    "    \n",
    "    #이미지의 라벨과 이미지 함께 저장..\n",
    "    \n",
    "    \n",
    "    \n",
    "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. data pipeline : batch size 만큼 가져와서 데이터셋으로 만드는 작업\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle().batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).shuffle().batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. build AlexNet model\n",
    "\n",
    "def AlexNet_model():\n",
    "    conv1 = convLayer(self.X, 11, 11, 4, 4, 96, \"conv1\", \"VALID\")\n",
    "    lrn1 = LRN(conv1, 2, 2e-05, 0.75, \"norm1\")\n",
    "    pool1 = maxPoolLayer(lrn1, 3, 3, 2, 2, \"pool1\", \"VALID\")\n",
    "\n",
    "    conv2 = convLayer(pool1, 5, 5, 1, 1, 256, \"conv2\", groups = 2)\n",
    "    lrn2 = LRN(conv2, 2, 2e-05, 0.75, \"lrn2\")\n",
    "    pool2 = maxPoolLayer(lrn2, 3, 3, 2, 2, \"pool2\", \"VALID\")\n",
    "\n",
    "    conv3 = convLayer(pool2, 3, 3, 1, 1, 384, \"conv3\")\n",
    "\n",
    "    conv4 = convLayer(conv3, 3, 3, 1, 1, 384, \"conv4\", groups = 2)\n",
    "\n",
    "    conv5 = convLayer(conv4, 3, 3, 1, 1, 256, \"conv5\", groups = 2)\n",
    "    pool5 = maxPoolLayer(conv5, 3, 3, 2, 2, \"pool5\", \"VALID\")\n",
    "\n",
    "    fcIn = tf.reshape(pool5, [-1, 256 * 6 * 6])\n",
    "    fc1 = fcLayer(fcIn, 256 * 6 * 6, 4096, True, \"fc6\")\n",
    "    dropout1 = dropout(fc1, self.KEEPPRO)\n",
    "    fc2 = fcLayer(dropout1, 4096, 4096, True, \"fc7\")\n",
    "    dropout2 = dropout(fc2, self.KEEPPRO)\n",
    "\n",
    "    self.fc3 = fcLayer(dropout2, 4096, self.CLASSNUM, True, \"fc8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개의 conv레이어와 3개 fc레이어 fc레이어 한뒤 dropout사용 2개의 dropout 사용\n",
    "3번째  3 4번째 conv에서 padding=\"same\"이며 maxpooling을 하지 않음\n",
    "2번째 conv레이어 부터 groups=2로 설정\n",
    "fcLayer는 다 True를 사용하여  return tf.nn.relu(out)을 사용\n",
    "\n",
    ">> 이 부분 출처 : https://github.com/qkrwogus1213/Classification_AlexNet/blob/master/AlexNet%EC%BD%94%EB%93%9C%EB%A6%AC%EB%B7%B0.md\n",
    "\n",
    "다시 할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 이미지 Augmentation 즉 그 이미지의 상/하/좌/우/원본 , 이들의 각각의 반전 까지 해서 총 10개의 데이터에서\n",
    "#예측치의 평균 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성능 평가\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=0.01, decay=5e-5, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
